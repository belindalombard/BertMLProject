{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/belindalombard/BertMLProject/blob/master/Relevance_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f60af366",
      "metadata": {
        "id": "f60af366"
      },
      "source": [
        "# Machine Annotation - BERTimbau - Relevance\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this research project, we will be utilizing the BERTimbau pretrained model. See citation below: \\\n",
        "\n",
        "@inproceedings { \\\n",
        "  souza2020bertimbau, \\\n",
        "  author    = {F{\\'a}bio Souza and Rodrigo Nogueira and Roberto Lotufo}, \\\n",
        "  title     = {{BERT}imbau: pretrained {BERT} models for {B}razilian {P}ortuguese}, \\\n",
        "  booktitle = {9th Brazilian Conference on Intelligent Systems, {BRACIS}, Rio Grande do Sul, Brazil, October 20-23 (to appear)}, \\\n",
        "  year      = {2020} \\\n",
        "}\n"
      ],
      "metadata": {
        "id": "jZdzHVsXWiTY"
      },
      "id": "jZdzHVsXWiTY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set Up\n",
        "Mount to drive to access files for training the model:"
      ],
      "metadata": {
        "id": "mHmTeQNnUszz"
      },
      "id": "mHmTeQNnUszz"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "UaYA4PulqHWG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "outputId": "f0225245-e4e0-40a9-9884-69a698b9ad77"
      },
      "id": "UaYA4PulqHWG",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "mount failed",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         )\n\u001b[0;32m--> 283\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: mount failed"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Libraries"
      ],
      "metadata": {
        "id": "TMVsslLUVBsn"
      },
      "id": "TMVsslLUVBsn"
    },
    {
      "cell_type": "markdown",
      "id": "b608a1eb",
      "metadata": {
        "id": "b608a1eb"
      },
      "source": [
        "Install transformers:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15d75e46",
      "metadata": {
        "id": "15d75e46",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import libraries:"
      ],
      "metadata": {
        "id": "zr3ZeX_IOnyz"
      },
      "id": "zr3ZeX_IOnyz"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cea81cc2",
      "metadata": {
        "id": "cea81cc2"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "from transformers import BertForSequenceClassification, BertTokenizer, pipeline\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset, random_split, Subset, RandomSampler, SequentialSampler, Dataset\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n",
        "import os\n",
        "import torch\n",
        "from torch.optim import AdamW\n",
        "import seaborn as sns\n",
        "import random\n",
        "import gc"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Package versions:"
      ],
      "metadata": {
        "id": "pA3iw9B7PHbW"
      },
      "id": "pA3iw9B7PHbW"
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Transformers version: {transformers.__version__}\")\n",
        "print(f\"Pandas version: {pd.__version__}\")\n",
        "print(f\"Numpy version: {np.__version__}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Scikit-learn version: {sklearn.__version__}\")\n",
        "print(f\"Seaborn version: {sns.__version__}\")"
      ],
      "metadata": {
        "id": "nWPQL1gdPLUu"
      },
      "id": "nWPQL1gdPLUu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Connect to GPU\n",
        "Connect to GPU if available:"
      ],
      "metadata": {
        "id": "8x_gB7fgzw5s"
      },
      "id": "8x_gB7fgzw5s"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available:\n",
        "if torch.cuda.is_available():\n",
        "\n",
        "    # Use GPU\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print(\"There are available GPU(s).\") # torch.cuda.device_cour\n",
        "\n",
        "    print(\"We will use the GPU:\", torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not\n",
        "\n",
        "else:\n",
        "    print(\"No GPU available, using CPU\")\n",
        "    device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "mRGhgm9drGq9"
      },
      "id": "mRGhgm9drGq9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GLOBAL VARIABLES - DATA\n",
        "FILE_PATH = path to data file that will be used \\\n",
        "\n",
        "RELEVANCE = name of column representing relevance \\\n",
        "RELEVANCE_ID = column value that represents (relevance == True) \\\n",
        "\n",
        "POST = name of column representing the content of the tweet \\\n",
        "POST_DATE = name of column representing the post date \\\n",
        "POST_ID = name of column representing the tweet id\n",
        "\n",
        "YEAR = year of when the dataset is from (to be used for file/folder naming)"
      ],
      "metadata": {
        "id": "8-3218n_elhK"
      },
      "id": "8-3218n_elhK"
    },
    {
      "cell_type": "code",
      "source": [
        "FILE_PATH = '/content/drive/MyDrive/pesquisa-usp/relevant_tweets_2020-12-31.csv'\n",
        "FOLDER_PATH = '/content/drive/MyDrive/pesquisa-usp/round-3/'\n",
        "\n",
        "RELEVANCE = 'relevance'\n",
        "RELEVANCE_ID = 'relevant'\n",
        "\n",
        "POST = 'content'\n",
        "POST_DATE = 'post_date'\n",
        "POST_ID = 'tweet_id'\n",
        "\n",
        "YEAR = '2020'\n"
      ],
      "metadata": {
        "id": "RQRRyNytek3K"
      },
      "id": "RQRRyNytek3K",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GLOBAL VARIABLES - MODEL"
      ],
      "metadata": {
        "id": "LuMMG3FLpYYy"
      },
      "id": "LuMMG3FLpYYy"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below are global variables related to training the model. \\\n",
        "This will make the code more maintainable and easier to experiment with new values."
      ],
      "metadata": {
        "id": "ej8R6M1HZNz2"
      },
      "id": "ej8R6M1HZNz2"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "MODEL_TYPE = 'neuralmind/bert-base-portuguese-cased'\n",
        "NUM_CLASSES = 2\n",
        "\n",
        "SEED_VAL = 42\n",
        "\n",
        "NUM_FOLDS = 5\n",
        "NUM_EPOCHS = 5\n",
        "\n",
        "L_RATE = 2e-5 # Testar novos valores\n",
        "MAX_LEN = 256 # Testar novos valores\n",
        "BATCH_SIZE = 32 # Testar novos valores\n",
        "EPS = 1e-8 # Testar novos valores\n",
        "# regression epsylon\n",
        "\n",
        "NUM_CORES = os.cpu_count()"
      ],
      "metadata": {
        "id": "3FG0mAtWtfut"
      },
      "id": "3FG0mAtWtfut",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load BERT"
      ],
      "metadata": {
        "id": "yppgDLFm0p8d"
      },
      "id": "yppgDLFm0p8d"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we will load BertForSequenceClassification. \\\n",
        "This is a pre-trained BERT model with a single linear classification layer on top."
      ],
      "metadata": {
        "id": "Vh0rp4uqdDbh"
      },
      "id": "Vh0rp4uqdDbh"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForSequenceClassification\n",
        "\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "          MODEL_TYPE,\n",
        "          num_labels = NUM_CLASSES,\n",
        "          output_attentions = False,\n",
        "          output_hidden_states = False\n",
        "        )\n",
        "\n",
        "\n",
        "# Send the model to the device:\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "9qpDYkk_0z9A"
      },
      "id": "9qpDYkk_0z9A",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "We will also save the path to this model so that it can be accessed at the start of the training loop.\\\n",
        "Variation of performance was observed when a new BertForSequenceClassification model is loaded.\n",
        "\n",
        "We plan to compare pre-training performance with post-training performance. \\\n",
        "Thus, we want to ensure that the model being trained is the same model that was previously evaluated."
      ],
      "metadata": {
        "id": "fWIq84iEgbhG"
      },
      "id": "fWIq84iEgbhG"
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'original_model.bin'\n",
        "\n",
        "torch.save(model.state_dict(), model_name)\n",
        "print('Saved model as', model_name)"
      ],
      "metadata": {
        "id": "VtsKLwFqhww4"
      },
      "id": "VtsKLwFqhww4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Tokenizer"
      ],
      "metadata": {
        "id": "c0wkGa3GveW5"
      },
      "id": "c0wkGa3GveW5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load BERT tokenizer:"
      ],
      "metadata": {
        "id": "IYS38S-Xc_bR"
      },
      "id": "IYS38S-Xc_bR"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained(MODEL_TYPE, do_lower_case=False)"
      ],
      "metadata": {
        "id": "vuSepgBmvjTX"
      },
      "id": "vuSepgBmvjTX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "83314bbe",
      "metadata": {
        "id": "83314bbe"
      },
      "source": [
        "## Prepare Dataset\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the cvs file of the dataset chosen to train the model:"
      ],
      "metadata": {
        "id": "gQ5oPenxfKon"
      },
      "id": "gQ5oPenxfKon"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ab7464e",
      "metadata": {
        "id": "8ab7464e",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "df = pd.read_csv(FILE_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run this cell to display the dataset:"
      ],
      "metadata": {
        "id": "SYOZdtrQfhJB"
      },
      "id": "SYOZdtrQfhJB"
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "collapsed": true,
        "id": "FQIgSbOBS5eF"
      },
      "id": "FQIgSbOBS5eF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "ebf081a9",
      "metadata": {
        "id": "ebf081a9"
      },
      "source": [
        "###  Parsing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display the class distribution of the dataset:"
      ],
      "metadata": {
        "id": "97hZQOF-UOXi"
      },
      "id": "97hZQOF-UOXi"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Tweet_Relevancies\", YEAR + \":\")\n",
        "\n",
        "print(df[RELEVANCE].value_counts())"
      ],
      "metadata": {
        "id": "8X1xE1R8HFP7"
      },
      "id": "8X1xE1R8HFP7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we will recode the class names to use integers instead of strings:"
      ],
      "metadata": {
        "id": "PH0_CMBSWEdq"
      },
      "id": "PH0_CMBSWEdq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af9d3e11",
      "metadata": {
        "id": "af9d3e11"
      },
      "outputs": [],
      "source": [
        "def condition(x):\n",
        "    if x == RELEVANCE_ID:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "df[RELEVANCE] = df[RELEVANCE].apply(condition)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will illustrate this recoding of class names by displaying the class distributions again:"
      ],
      "metadata": {
        "id": "64Z9jrRRgNyZ"
      },
      "id": "64Z9jrRRgNyZ"
    },
    {
      "cell_type": "code",
      "source": [
        "print(df[RELEVANCE].value_counts())"
      ],
      "metadata": {
        "id": "UepfLsx7gfuj"
      },
      "id": "UepfLsx7gfuj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot overall class distribution and save the graph as a png:"
      ],
      "metadata": {
        "id": "zEwndYjNiFxi"
      },
      "id": "zEwndYjNiFxi"
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(5, 5))\n",
        "ax = sns.countplot(df,\n",
        "                   x=RELEVANCE,\n",
        "                   hue=RELEVANCE,\n",
        "                   palette=\"Paired\",\n",
        "                   width=0.3,\n",
        "                   legend=False\n",
        "                  )\n",
        "\n",
        "plt.title(\"Class Distribution - Whole Dataset\",fontsize=22)\n",
        "plt.xlabel('Relevance',fontsize=12)\n",
        "plt.ylabel('Count',fontsize=12)\n",
        "\n",
        "plt.xticks([0,1],['not relevant', 'relevant'])\n",
        "\n",
        "plt.grid(True,axis='y',\n",
        "         linestyle=':',\n",
        "         linewidth=0.5\n",
        "        )\n",
        "\n",
        "for i in ax.containers:\n",
        "    ax.bar_label(i,)\n",
        "\n",
        "\n",
        "plt.savefig(FOLDER_PATH+'ClassDistribution_WholeDataset.png',\n",
        "            format='png',\n",
        "            transparent = True,\n",
        "            bbox_inches='tight',\n",
        "            pad_inches=0.1\n",
        "           )"
      ],
      "metadata": {
        "id": "Wym1M45NiJyP"
      },
      "id": "Wym1M45NiJyP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split Train/Test Sets"
      ],
      "metadata": {
        "id": "C-dYohVKoByW"
      },
      "id": "C-dYohVKoByW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we separate **10%** of the total observations for the **test set**. \\\\\n",
        "We use train_test_split so that the **data is shuffled** before splitting, using a seed (SEED_VAL) for the random state. \\\\\n",
        "We use a stratifier to ensure that **class distribution proportions are maintained** in both sets. \\\\\n",
        "\n",
        "**NOTE:** Train and Test sets should be tokenized and processed separately to avoid data leakage. \\\\"
      ],
      "metadata": {
        "id": "Dvx2_VjBSKfA"
      },
      "id": "Dvx2_VjBSKfA"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df_train, df_test = train_test_split(df,\n",
        "                                     test_size = 0.1, # 10% for the test set\n",
        "                                     stratify = df[RELEVANCE], # maintain class proportions\n",
        "                                     random_state = SEED_VAL # ensures reproducibility of results\n",
        "                                    )"
      ],
      "metadata": {
        "id": "2HiVYUuDoE6F"
      },
      "id": "2HiVYUuDoE6F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display class distributions of each set:"
      ],
      "metadata": {
        "id": "-zHrDbVdYVSm"
      },
      "id": "-zHrDbVdYVSm"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training Set Class Distribution:\")\n",
        "print(df_train[RELEVANCE].value_counts())\n",
        "print(\"\\nTest Set Class Distribution:\")\n",
        "print(df_test[RELEVANCE].value_counts())"
      ],
      "metadata": {
        "id": "MqhHvh5MYX6T"
      },
      "id": "MqhHvh5MYX6T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we plot class distributions across training/test sets and save graph as a png. \\\n",
        "We add the DATASET column for plotting purposes, it will not be relevant later."
      ],
      "metadata": {
        "id": "Y9Hv4XHUjRK8"
      },
      "id": "Y9Hv4XHUjRK8"
    },
    {
      "cell_type": "code",
      "source": [
        "df_train['DATASET'] = 'Train'\n",
        "df_test['DATASET'] = 'Test'\n",
        "\n",
        "# Combine the two DataFrames\n",
        "df_combined = pd.concat([df_train, df_test])\n",
        "\n",
        "# Plot class distributions in a single plot\n",
        "plt.figure(figsize=(5, 5))\n",
        "ax = sns.countplot(x='DATASET',\n",
        "                   hue=RELEVANCE,\n",
        "                   data=df_combined,\n",
        "                   palette=\"Paired\",\n",
        "                   width=0.5\n",
        "                  )\n",
        "\n",
        "plt.title('Class Distribution - Train/Test Sets', fontsize=22)\n",
        "plt.xlabel('Dataset',fontsize=12)\n",
        "plt.ylabel('Count',fontsize=12)\n",
        "plt.legend(title='Relevance', labels=['not relevant', 'relevant'])\n",
        "\n",
        "plt.grid(True,axis='y',\n",
        "         linestyle=':',\n",
        "         linewidth=0.5\n",
        "        )\n",
        "\n",
        "for i in ax.containers:\n",
        "    ax.bar_label(i,)\n",
        "\n",
        "plt.savefig(FOLDER_PATH+'ClassDistribution_TrainTestSplit.png',\n",
        "            format='png',\n",
        "            transparent = True,\n",
        "            bbox_inches='tight',\n",
        "            pad_inches=0.1\n",
        "           )"
      ],
      "metadata": {
        "id": "4__zHtJMjXVS"
      },
      "id": "4__zHtJMjXVS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom Dataset Class"
      ],
      "metadata": {
        "id": "Ctc1ZTW3olhK"
      },
      "id": "Ctc1ZTW3olhK"
    },
    {
      "cell_type": "markdown",
      "source": [
        "This class will be used to tokenize sentences:"
      ],
      "metadata": {
        "id": "hJfsakvNj96D"
      },
      "id": "hJfsakvNj96D"
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class TweetDataset(Dataset):\n",
        "  def __init__(self, df):\n",
        "    self.df_data = df\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    # Get the sentences from the dataframe\n",
        "    sentence = self.df_data.iloc[index][POST]\n",
        "\n",
        "    # Transform the sentence\n",
        "\n",
        "    encoded_dict = tokenizer.encode_plus(sentence,  # Sentences to be tokenized\n",
        "                                         add_special_tokens = True, # [CLS] and [SEP]\n",
        "                                         padding = 'max_length',\n",
        "                                         max_length = MAX_LEN,\n",
        "                                         return_attention_mask = True, # Build attn. masks.\n",
        "                                         return_tensors = 'pt', # Return pytorch tensors\n",
        "                                        )\n",
        "\n",
        "\n",
        "    # Torch Tensors\n",
        "\n",
        "    padded_token_list = encoded_dict['input_ids'][0]\n",
        "    att_mask = encoded_dict['attention_mask'][0]\n",
        "    token_type_ids = encoded_dict['token_type_ids'][0]\n",
        "\n",
        "\n",
        "    # Transforming the Target into a tensor\n",
        "\n",
        "    target = torch.tensor(self.df_data.iloc[index][RELEVANCE])\n",
        "\n",
        "    sample = (padded_token_list, att_mask, token_type_ids, target)\n",
        "\n",
        "    return sample\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "\n",
        "    return len(self.df_data)"
      ],
      "metadata": {
        "id": "RT4jFm7Hoxm1"
      },
      "id": "RT4jFm7Hoxm1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Test Dataloader"
      ],
      "metadata": {
        "id": "NzSknRaEuKrK"
      },
      "id": "NzSknRaEuKrK"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we create an object of the TweetDataset to tokenize all sentences in our curated dataset. \\\n",
        "We then create a DataLoader object to split the test data into batches, which will be used for the testing loop later one."
      ],
      "metadata": {
        "id": "Bc0SIfk0kJSE"
      },
      "id": "Bc0SIfk0kJSE"
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "df_test.reset_index(drop=True, inplace=True)\n",
        "test_data = TweetDataset(df_test)\n",
        "\n",
        "test_dataloader = DataLoader(test_data,\n",
        "                             batch_size=BATCH_SIZE,\n",
        "                             shuffle=False,\n",
        "                             num_workers=NUM_CORES\n",
        "                            )\n",
        "\n",
        "print('Test Dataloader Length:',len(test_dataloader))\n",
        "print('\\nIndex Range of Test Dataloader:', df_test.index)\n",
        "print('\\nFirst 5 Rows of Test Dataloader:')\n",
        "print(df_test.head())\n"
      ],
      "metadata": {
        "id": "1aPhljZ4ph6R"
      },
      "id": "1aPhljZ4ph6R",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metrics Function"
      ],
      "metadata": {
        "id": "K5d0UAxx7OLT"
      },
      "id": "K5d0UAxx7OLT"
    },
    {
      "cell_type": "markdown",
      "source": [
        "To return evaluation scores, we will use the **classification_report_with_accuracy_score** as defined below."
      ],
      "metadata": {
        "id": "iUZqCDy7mzUA"
      },
      "id": "iUZqCDy7mzUA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Arguments: \\\n",
        "- **preds** = a flat list of the predictions of the model as integers\n",
        "- **labels** = a flat list of the true classification of the corresponding predictions\n",
        "\n",
        "Returns: \\\n",
        "- **df_class_report** = a DataFrame of the classification report made using **preds** and **labels**, which includes:\n",
        "\n",
        " - *precision*, *recall*, and *F1-score* for each class.\n",
        "\n",
        " - *macro average* and *weighted average* between both classes for precision, recall and F1-score.\n",
        "\n",
        "\n",
        " - **overall_accuracy** = overall *accuracy* of the model.\n",
        "\n",
        " All metrics are rounded to the closest 2 decimals."
      ],
      "metadata": {
        "id": "ZnewCz6frxMG"
      },
      "id": "ZnewCz6frxMG"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def classification_report_with_accuracy_score(preds, labels):\n",
        "    report = classification_report(labels, preds, output_dict = True, zero_division = 0)\n",
        "\n",
        "    df_class_report = pd.DataFrame(report).transpose()\n",
        "    df_class_report = df_class_report.round(decimals=2) # for better readibility\n",
        "\n",
        "    overall_accuracy = np.sum(preds == labels) / len(labels)\n",
        "    rounded_accuracy = overall_accuracy.round(decimals=2)\n",
        "\n",
        "    return df_class_report, rounded_accuracy"
      ],
      "metadata": {
        "id": "s9IkFODb7V_K"
      },
      "id": "s9IkFODb7V_K",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate the BERTimbau Pre-Trained Model\n",
        "\n"
      ],
      "metadata": {
        "id": "nljQ_ztLt-8J"
      },
      "id": "nljQ_ztLt-8J"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we will evaluate how BERTimbau (a pre-trained model) performs before being trained on our curated dataset. \\\n",
        "We refer to the process of specifically training BERTimbau on our dataset as fine-tuning, which will be done later on.  \n",
        "\n",
        "\\\\\n",
        "We will evaluate performance using the following metrics: \\\\\n",
        "- **Accuracy**: correct predictions/total number of instances\n",
        "- **Precision**: percentage of instances that the model classified correctly for a given class\n",
        "- **Recall**: percentage of instances of a given class that were classified correctly\n",
        "- **F1-Score**: harmonic mean of precision and recall\n",
        "- **Confusion Matrix**: represents number of actual outputs versus predicted outputs\n"
      ],
      "metadata": {
        "id": "lVVwwBD0DRGq"
      },
      "id": "lVVwwBD0DRGq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Test batches"
      ],
      "metadata": {
        "id": "ZQ_DR5iV9EYM"
      },
      "id": "ZQ_DR5iV9EYM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the test_dataloader to evaluate BERTimbau's performance, testing in batches:"
      ],
      "metadata": {
        "id": "NL4wCVGPrgKk"
      },
      "id": "NL4wCVGPrgKk"
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "for i, test_batch in enumerate(test_dataloader):\n",
        "  test_status = 'Batch ' + str(i+1) + ' / ' + str(len(test_dataloader))\n",
        "  print(test_status, end='\\r')\n",
        "\n",
        "  b_input_ids = test_batch[0].to(device)\n",
        "  b_input_mask = test_batch[1].to(device)\n",
        "  b_token_type_ids = test_batch[2].to(device)\n",
        "\n",
        "\n",
        "  outputs = model(b_input_ids,\n",
        "                  token_type_ids=None,\n",
        "                  attention_mask=b_input_mask,\n",
        "                  return_dict = False)\n",
        "\n",
        "  preds = outputs[0]\n",
        "  test_preds = preds.detach().cpu().numpy()\n",
        "\n",
        "\n",
        "  if i == 0:  # first batch\n",
        "    stacked_test_preds = test_preds\n",
        "\n",
        "  else:\n",
        "    stacked_test_preds = np.vstack((stacked_test_preds, test_preds))\n"
      ],
      "metadata": {
        "id": "vFT0kkkH326f"
      },
      "id": "vFT0kkkH326f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each prediction returned by the model is a tuple - the logits are still raw.\n",
        "\n",
        "The first value of the tuple represents the score given for Class 0, and the second value represents the score given for Class 1. \\\\\n",
        "To get the real prediction, we take the max of both values to know whether the model's final prediction was Class 0 or Class 1. \\\\\n",
        "\n",
        "For clarity, we will illustrate this below:"
      ],
      "metadata": {
        "id": "2hkM89YPkFZv"
      },
      "id": "2hkM89YPkFZv"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"First 5 predictions as a raw logit tuple:\\n\", stacked_test_preds[0:6])\n",
        "final_preds = np.argmax(stacked_test_preds, axis=1)\n",
        "print(\"\\nFirst 5 predictions final decisions after taking the max:\\n\", final_preds[0:6])\n"
      ],
      "metadata": {
        "id": "6uBBZ_Kqa9wX"
      },
      "id": "6uBBZ_Kqa9wX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Performance Scores"
      ],
      "metadata": {
        "id": "4OVUNPioyMbX"
      },
      "id": "4OVUNPioyMbX"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can calculate the model's performance scores using the function that was previously defined. \\"
      ],
      "metadata": {
        "id": "Ik6aUaSGtODD"
      },
      "id": "Ik6aUaSGtODD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "It must be noted that when reloading the model and rerunning the tests, a small fluctuation of values was observed in the performance scores. \\\n",
        "It was ensured that the Train/Test split of the dataset and the tokenization of the test set were the same in all trials.\n"
      ],
      "metadata": {
        "id": "UKMq_hWNpIrm"
      },
      "id": "UKMq_hWNpIrm"
    },
    {
      "cell_type": "code",
      "source": [
        "labels = df_test[RELEVANCE]\n",
        "\n",
        "report, accuracy = classification_report_with_accuracy_score(final_preds, labels)\n",
        "\n",
        "print(report)"
      ],
      "metadata": {
        "id": "p2OhVqajsaZU"
      },
      "id": "p2OhVqajsaZU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot report as a table and save figure as a png:"
      ],
      "metadata": {
        "id": "1mFDCp40fwkb"
      },
      "id": "1mFDCp40fwkb"
    },
    {
      "cell_type": "code",
      "source": [
        "without_acc = report.loc[['0', '1', 'macro avg', 'weighted avg']]\n",
        "without_acc = without_acc.rename(index={'0': 'not relevant', '1': 'relevant'})\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(2,4))\n",
        "ax.axis('off')\n",
        "table = pd.plotting.table(ax,\n",
        "                          without_acc,\n",
        "                          loc='right',\n",
        "                          cellLoc='center',\n",
        "                          rowLoc='center'\n",
        "                         )\n",
        "\n",
        "table.auto_set_font_size(False)\n",
        "table.set_fontsize(14)\n",
        "table.scale(2.9, 3)\n",
        "plt.title('BERTimbau Performance Before Fine-Tuning',\n",
        "          loc='left',\n",
        "          fontsize=20\n",
        "         )\n",
        "\n",
        "acc_str = 'Overall Accuracy: '+ str(accuracy)\n",
        "fig.text(0.67, 0.08,\n",
        "         acc_str,\n",
        "         ha='center',\n",
        "         fontsize=14\n",
        "        )\n",
        "\n",
        "\n",
        "plt.savefig(FOLDER_PATH+'PerformanceTable_PreTrainedModel.png',\n",
        "            format='png',\n",
        "            transparent = True,\n",
        "            bbox_inches='tight',\n",
        "            pad_inches=0.1\n",
        "           )"
      ],
      "metadata": {
        "id": "LSIjveam0ymh"
      },
      "id": "LSIjveam0ymh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the confusion matrix to visualize the model's predictions against their true value, and save the figure as a png:"
      ],
      "metadata": {
        "id": "1v77GE8svzV9"
      },
      "id": "1v77GE8svzV9"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "confusion_matrix = confusion_matrix(df_test[RELEVANCE], final_preds)\n",
        "\n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "\n",
        "ax = sns.heatmap(confusion_matrix,\n",
        "                 annot=True,\n",
        "                 cmap=\"rocket_r\",\n",
        "                 fmt = '.0f',\n",
        "                 yticklabels=['not relevant', 'relevant'],\n",
        "                 xticklabels=['not relevant', 'relevant'],\n",
        "                 square=True,\n",
        "                 annot_kws={\"size\": 14},\n",
        "                 cbar=True,\n",
        "                 cbar_kws={'shrink': 0.9}\n",
        "                )\n",
        "\n",
        "plt.xticks(fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "\n",
        "# Add title and labels\n",
        "plt.title('BERTimbau Performance Before Fine-Tuning',\n",
        "          fontsize = 22,\n",
        "          loc = 'center',\n",
        "          pad = 20\n",
        "         )\n",
        "\n",
        "plt.xlabel('Predicted Labels', fontsize=16)\n",
        "plt.ylabel('True Labels', fontsize=16)\n",
        "\n",
        "plt.savefig(FOLDER_PATH+'ConfusionMatrix_PreTrainedModel.png',\n",
        "            format='png',\n",
        "            transparent = True,\n",
        "            bbox_inches='tight',\n",
        "            pad_inches=0.1\n",
        "           )\n"
      ],
      "metadata": {
        "id": "qRJ6Wc90H51c"
      },
      "id": "qRJ6Wc90H51c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train Model"
      ],
      "metadata": {
        "id": "c_iafbTvuuvn"
      },
      "id": "c_iafbTvuuvn"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now train BERTimbau on our curated dataset.\\\n",
        "The goal is to achieve significantly better evaluation scores compared to the pre-trained model's performance."
      ],
      "metadata": {
        "id": "AeAhjhZuwIF-"
      },
      "id": "AeAhjhZuwIF-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Set the Seed"
      ],
      "metadata": {
        "id": "crarghCovW58"
      },
      "id": "crarghCovW58"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The seed will ensure reproducibility of these experiments:"
      ],
      "metadata": {
        "id": "-UThqikvx2X0"
      },
      "id": "-UThqikvx2X0"
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(SEED_VAL)\n",
        "np.random.seed(SEED_VAL)\n",
        "torch.manual_seed(SEED_VAL)\n",
        "torch.cuda.manual_seed_all(SEED_VAL)"
      ],
      "metadata": {
        "id": "I1pKuz5VvcTA"
      },
      "id": "I1pKuz5VvcTA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Remove Warnings"
      ],
      "metadata": {
        "id": "vBqYaQxtDxTi"
      },
      "id": "vBqYaQxtDxTi"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AdamW\n",
        "import random\n",
        "import gc\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "# Remove Warnings\n",
        "\n",
        "from transformers import logging\n",
        "\n",
        "logging.set_verbosity_warning()"
      ],
      "metadata": {
        "id": "csWHPk4l9G1o"
      },
      "id": "csWHPk4l9G1o",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Create Folds"
      ],
      "metadata": {
        "id": "4Ua3_mcAu57a"
      },
      "id": "4Ua3_mcAu57a"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to train the model, we must split the training dataset into train/validation sets.\\\n",
        "To do this, we will create folds such that each fold will have a different randomized (but seed controlled) dataset split."
      ],
      "metadata": {
        "id": "4CMluaX9u9Rf"
      },
      "id": "4CMluaX9u9Rf"
    },
    {
      "cell_type": "code",
      "source": [
        "skf = StratifiedKFold(n_splits = NUM_FOLDS,\n",
        "                     shuffle = True,\n",
        "                     random_state = SEED_VAL)\n",
        "\n",
        "targets = df_train[RELEVANCE]\n",
        "\n",
        "folds_list = skf.split(df_train,targets)"
      ],
      "metadata": {
        "id": "ySIZ9marCuwc"
      },
      "id": "ySIZ9marCuwc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Class Distribution Across Folds"
      ],
      "metadata": {
        "id": "Cg1JDeNqKLxp"
      },
      "id": "Cg1JDeNqKLxp"
    },
    {
      "cell_type": "code",
      "source": [
        "# create a figure with subplots\n",
        "fig, axes = plt.subplots(1,NUM_FOLDS, figsize=(18, 8))\n",
        "fig.suptitle('Class Distributions Across Folds', fontsize=18)\n",
        "\n",
        "for n, (train_index, val_index) in enumerate(skf.split(df_train,targets)):\n",
        "  train = df_train.iloc[train_index,:]\n",
        "  validation = df_train.iloc[val_index,:]\n",
        "\n",
        "  # Group the datasets to be plotted:\n",
        "  train.loc[:, 'DATASET'] = 'Train'\n",
        "  validation.loc[:, 'DATASET'] = 'Validation'\n",
        "  df_combined = pd.concat([train, validation])\n",
        "  # plt.figure(figsize=(5, 5))\n",
        "\n",
        "  sub_fig = axes[n]\n",
        "  ax = sns.countplot(ax=sub_fig, x='DATASET', hue=RELEVANCE, data=df_combined, palette=\"Paired\", width=0.5)\n",
        "\n",
        "  title = 'Fold ' + str(n+1)\n",
        "  sub_fig.set_title(title, fontsize=14)\n",
        "\n",
        "  # Set labels and legend\n",
        "  sub_fig.set_xlabel('Dataset', fontsize=12)\n",
        "  sub_fig.set_ylabel('')\n",
        "  sub_fig.legend(title='Relevance', loc='upper right')\n",
        "  sub_fig.grid(True, axis='y', linestyle=':', linewidth=0.5)\n",
        "\n",
        "\n",
        "  for i in ax.containers:\n",
        "      ax.bar_label(i,)\n",
        "\n",
        "\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.savefig(FOLDER_PATH+'ClassDistribution_AcrossFolds.png', format='png', transparent = True, bbox_inches='tight',pad_inches=0.1)"
      ],
      "metadata": {
        "id": "wnkJcGFTJO5h"
      },
      "id": "wnkJcGFTJO5h",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Lists for Performance Scores/Data"
      ],
      "metadata": {
        "id": "MZgpmbmLF0Cw"
      },
      "id": "MZgpmbmLF0Cw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the list below to store performance scores of all epochs across all folds.\n",
        "\n",
        "Each epoch is a dictionary containing:\n",
        "- fold number\n",
        "- epoch number\n",
        "\n",
        "- total train loss\n",
        "- average train loss\n",
        "- total validation loss\n",
        "- average validation loss\n",
        "\n",
        "- classification report\n",
        "- validation accuracy"
      ],
      "metadata": {
        "id": "ErBSYvttQnyw"
      },
      "id": "ErBSYvttQnyw"
    },
    {
      "cell_type": "code",
      "source": [
        "training_stats = []"
      ],
      "metadata": {
        "id": "YzkFfbnuO1h0"
      },
      "id": "YzkFfbnuO1h0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Training Loop"
      ],
      "metadata": {
        "id": "roi36bWDFob9"
      },
      "id": "roi36bWDFob9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We train the model in NUM_FOLDS folds, each of which will have NUM_EPOCHS epochs."
      ],
      "metadata": {
        "id": "Sn3bafuZAXav"
      },
      "id": "Sn3bafuZAXav"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Each fold loads a new pre-trained BERTimbau model to fine-tune. \\\n",
        "\n",
        "Each epoch in this fold will train the model on the fold's train set, followed by an evaluation of the model using the fold's validation set. \\\n",
        "\\\n",
        "If its Macro F1-score is better than the other epochs, that model will be saved using: \\\n",
        "  - model_name = 'fold_' + str(fold_index) + '.bin'\n",
        "  - torch.save(model.state_dict(), model_name)\n",
        "\n",
        "We will also use a variable to store the name of the best model out of all folds, so that we can load it later for testing.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KP9mSbhH7cKB"
      },
      "id": "KP9mSbhH7cKB"
    },
    {
      "cell_type": "code",
      "source": [
        "# best_fold_path = ''"
      ],
      "metadata": {
        "id": "YEbCDboRqoaJ"
      },
      "id": "YEbCDboRqoaJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "for fold_index, (train_index, val_index) in enumerate(skf.split(df_train,targets)):\n",
        "  print('\\n################## Fold Model Number', str((fold_index+1)) + ' / ' + str(NUM_FOLDS), '##################')\n",
        "\n",
        "  # ..................................................\n",
        "  # Defining Dataloaders of Training and Validation\n",
        "  # ..................................................\n",
        "\n",
        "  # Train and Validation Sets\n",
        "  train = df_train.iloc[train_index,:]\n",
        "  val = df_train.iloc[val_index,:]\n",
        "\n",
        "\n",
        "  # Reseting Indices so it will work\n",
        "  train = train.reset_index(drop = True)\n",
        "  val = val.reset_index(drop = True)\n",
        "\n",
        "\n",
        "  # Tokenizing and Transforming Inputs (att masks, labels, etc.)\n",
        "  train_data = TweetDataset(train)\n",
        "  val_data = TweetDataset(val)\n",
        "\n",
        "  # Train/Validation Split for this Fold:\n",
        "  train_dataloader = torch.utils.data.DataLoader(train_data,\n",
        "                                                 batch_size=BATCH_SIZE,\n",
        "                                                 shuffle=False,\n",
        "                                                 num_workers=NUM_CORES)\n",
        "\n",
        "  val_dataloader = torch.utils.data.DataLoader(val_data,\n",
        "                                               batch_size=BATCH_SIZE,\n",
        "                                               shuffle=False,\n",
        "                                               num_workers=NUM_CORES)\n",
        "\n",
        "  # ..................................................\n",
        "  # List for Epoch Evaluation Metric:\n",
        "  # ..................................................\n",
        "  fold_f1_macro_list = []\n",
        "\n",
        "#------------------------------------------------------------------------------\n",
        "  for epoch in range(NUM_EPOCHS):\n",
        "#------------------------------------------------------------------------------\n",
        "\n",
        "    #......................\n",
        "    # Loading Fold Model\n",
        "    #......................\n",
        "\n",
        "    if epoch == 0:\n",
        "\n",
        "      # Load model from pre-training evaluation:\n",
        "      model.load_state_dict(torch.load('original_model.bin'))\n",
        "\n",
        "      optimizer = AdamW(model.parameters(),\n",
        "                        lr = L_RATE,\n",
        "                        eps = EPS\n",
        "                       )\n",
        "\n",
        "    else:\n",
        "      # Load the fold's model:\n",
        "      path_model = 'fold_' + str(fold_index) + '.bin'\n",
        "\n",
        "      if os.path.exists(path_model):\n",
        "        model.load_state_dict(torch.load(path_model))\n",
        "\n",
        "      else:\n",
        "        print(f\"Error: {path_model} does not exist\")\n",
        "\n",
        "        model.to(device)\n",
        "\n",
        "\n",
        "    # ==========================================================================\n",
        "    #               Training\n",
        "    # ==========================================================================\n",
        "    print('============== Epoch {:} / {:} =============='.format(epoch + 1, NUM_EPOCHS))\n",
        "    print(\" \")\n",
        "    print('\\nTraining...')\n",
        "    print('')\n",
        "\n",
        "    # Put the model into train mode:\n",
        "    model.train()\n",
        "\n",
        "    # Turn gradient calculations on:\n",
        "    torch.set_grad_enabled(True)\n",
        "\n",
        "\n",
        "    # .......................................................\n",
        "    # Defining Variables to Track This Epoch's Performance\n",
        "    # .......................................................\n",
        "\n",
        "    # overall stats:\n",
        "    epoch_stats = {'fold': fold_index + 1,\n",
        "                   'epoch': epoch + 1}\n",
        "\n",
        "    total_macro_f1 = 0\n",
        "    avg_macro_f1 = 0\n",
        "\n",
        "    # train stats:\n",
        "    total_train_loss = 0\n",
        "    avg_train_loss = 0\n",
        "\n",
        "    # validation stats:\n",
        "    total_val_loss = 0\n",
        "    avg_val_loss = 0\n",
        "\n",
        "    # ..................\n",
        "    # Train the Model\n",
        "    # ..................\n",
        "\n",
        "    for i, batch in enumerate(train_dataloader):\n",
        "\n",
        "      train_status = 'Batch ' + str(i+1) + ' of ' + str(len(train_dataloader))\n",
        "\n",
        "      print(train_status, end='\\r')\n",
        "\n",
        "\n",
        "      b_input_ids = batch[0].to(device)\n",
        "      b_input_mask = batch[1].to(device)\n",
        "      b_token_type_ids = batch[2].to(device)\n",
        "      b_labels = batch[3].to(device)\n",
        "\n",
        "      model.zero_grad()\n",
        "\n",
        "\n",
        "      loss, logits = model(b_input_ids,\n",
        "                           token_type_ids=None,\n",
        "                           attention_mask=b_input_mask,\n",
        "                           return_dict = False,\n",
        "                           labels=b_labels)\n",
        "\n",
        "      # Calculate loss:\n",
        "      batch_loss = loss.item()\n",
        "\n",
        "      total_train_loss += batch_loss\n",
        "\n",
        "\n",
        "      # Zero the gradients:\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # Perform a backward pass to calculate the gradients:\n",
        "      loss.backward()\n",
        "\n",
        "\n",
        "      # Clip the norm of the gradients to 1.0.\n",
        "      # This is to help prevent the \"exploding gradients\" problem.\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "\n",
        "      # Update Weights:\n",
        "      optimizer.step()   # for GPU\n",
        "\n",
        "      # https://pytorch.org/xla/\n",
        "      #xm.optimizer_step(optimizer, barrier=True)   #for TPU\n",
        "\n",
        "\n",
        "    # ........................................\n",
        "    # Update Training Performance Variables\n",
        "    # ........................................\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "\n",
        "    print('Total Train loss:' ,total_train_loss)\n",
        "    print('Average Train Loss: ', avg_train_loss)\n",
        "\n",
        "\n",
        "    # .......................................\n",
        "    # Add Scores to Epoch Stats Dictionary\n",
        "    # .......................................\n",
        "\n",
        "    epoch_stats['total_train_loss'] = total_train_loss\n",
        "    epoch_stats['avg_train_loss'] = avg_train_loss\n",
        "\n",
        "\n",
        "\n",
        "    # ==========================================================================\n",
        "    #               Validation\n",
        "    # ==========================================================================\n",
        "    print(\" \")\n",
        "    print('=============================================')\n",
        "    print('\\nValidation...')\n",
        "    print(' ')\n",
        "\n",
        "    # Put the model in evaluation mode:\n",
        "    model.eval()\n",
        "\n",
        "    # Turn off the gradient calculations.\n",
        "    # This tells the model not to compute or store gradients.\n",
        "    # This step saves memory and speeds up validation.\n",
        "    torch.set_grad_enabled(False)\n",
        "\n",
        "\n",
        "    # Lists to accumulate all predictions and their corresponding labels:\n",
        "    stacked_val_preds = []\n",
        "    val_targets = []\n",
        "\n",
        "\n",
        "    # .....................\n",
        "    # Validate the Model\n",
        "    # .....................\n",
        "\n",
        "    for j, val_batch in enumerate(val_dataloader):\n",
        "\n",
        "      val_status = 'Batch ' + str(j+1) + ' of ' + str(len(val_dataloader))\n",
        "\n",
        "      print(val_status, end='\\r')\n",
        "\n",
        "\n",
        "      b_input_ids = val_batch[0].to(device)\n",
        "      b_input_mask = val_batch[1].to(device)\n",
        "      b_token_type_ids = val_batch[2].to(device)\n",
        "      b_labels = val_batch[3].to(device)\n",
        "\n",
        "\n",
        "      loss, logits = model(b_input_ids,\n",
        "                           token_type_ids=None,\n",
        "                           attention_mask=b_input_mask,\n",
        "                           return_dict = False,\n",
        "                           labels=b_labels)\n",
        "\n",
        "\n",
        "      # Calculate loss:\n",
        "      batch_loss = loss.item()\n",
        "      total_val_loss += batch_loss\n",
        "\n",
        "\n",
        "      # Move labels and predictions to the CPU:\n",
        "      batch_targets = b_labels.to('cpu').numpy()\n",
        "      batch_preds = logits.detach().cpu().numpy()\n",
        "\n",
        "\n",
        "      # Append the labels and predictions to their lists:\n",
        "      val_targets.extend(batch_targets)\n",
        "\n",
        "      if j == 0:\n",
        "        stacked_val_preds = batch_preds\n",
        "      else:\n",
        "        stacked_val_preds = np.vstack((stacked_val_preds, batch_preds))\n",
        "\n",
        "\n",
        "      batch_preds = np.argmax(batch_preds, axis = 1)\n",
        "\n",
        "      total_macro_f1 += f1_score(batch_targets,\n",
        "                                 batch_preds,\n",
        "                                 average = 'macro'\n",
        "                                )\n",
        "      # usage: sklearn.metrics.f1_score(y_true, y_pred)\n",
        "\n",
        "\n",
        "    # ..........................................\n",
        "    # Update Validation Performance Variables\n",
        "    # ..........................................\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
        "\n",
        "    avg_macro_f1 = total_macro_f1 / len(val_dataloader)\n",
        "\n",
        "\n",
        "    print('Total Validation loss:' ,total_val_loss)\n",
        "    print('Average Validation Loss: ', avg_val_loss)\n",
        "\n",
        "    print(\"Macro F1-score: {0:.2f}\".format(avg_macro_f1))\n",
        "\n",
        "\n",
        "\n",
        "    stacked_val_preds = np.argmax(stacked_val_preds, axis=1)\n",
        "\n",
        "    class_report, val_acc = classification_report_with_accuracy_score(stacked_val_preds, val_targets)\n",
        "\n",
        "\n",
        "    print('Validation accuracy: ', val_acc)\n",
        "    print('Classification Report: \\n', class_report)\n",
        "\n",
        "\n",
        "\n",
        "    # .......................................\n",
        "    # Add Scores to Epoch Stats Dictionary\n",
        "    # .......................................\n",
        "    epoch_stats['total_val_loss'] = total_val_loss\n",
        "    epoch_stats['avg_val_loss'] = avg_val_loss\n",
        "    epoch_stats['val_accuracy'] = val_acc\n",
        "    epoch_stats['report'] = class_report\n",
        "\n",
        "    # ......................\n",
        "    # Save the best model\n",
        "    # ......................\n",
        "\n",
        "    if epoch == 0:\n",
        "\n",
        "      # Save the Model:\n",
        "      model_name = 'fold_' + str(fold_index) + '.bin'\n",
        "      torch.save(model.state_dict(), model_name)\n",
        "      print('Saved model as', model_name)\n",
        "\n",
        "    else:\n",
        "\n",
        "\n",
        "      # Get the best Macro-F1 score seen so far:\n",
        "      best_f1_macro = max(fold_f1_macro_list)\n",
        "\n",
        "      # Keep the model with the best Macro-F1 score:\n",
        "      if (avg_macro_f1 > best_f1_macro) and (avg_val_loss <= avg_train_loss):\n",
        "\n",
        "        model_name = 'fold_' + str(fold_index) + '.bin'\n",
        "        torch.save(model.state_dict(), model_name)\n",
        "\n",
        "        # best_fold_path = model_name\n",
        "        print('F1-Macro improved. Saved model as', model_name)\n",
        "\n",
        "\n",
        "        epoch_stats['f1_macro'] = avg_macro_f1\n",
        "\n",
        "\n",
        "    # Add the Macro-F1 scores of this epoch to the overall list:\n",
        "    fold_f1_macro_list.append(avg_macro_f1)\n",
        "\n",
        "    # .........................................\n",
        "    # Add Epoch Stats to Training Stats List\n",
        "    # .........................................\n",
        "    training_stats.append(epoch_stats)\n",
        "\n",
        "\n",
        "    # Use the garbage collector to save memory:\n",
        "    gc.collect()\n"
      ],
      "metadata": {
        "id": "tlWoSAxkFrqC"
      },
      "id": "tlWoSAxkFrqC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model Evaluation"
      ],
      "metadata": {
        "id": "biwKevXu_9jg"
      },
      "id": "biwKevXu_9jg"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare Data"
      ],
      "metadata": {
        "id": "tCKMSkWrloM1"
      },
      "id": "tCKMSkWrloM1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert the training_stats dictionary into a dataframe that will be used for data visualization:"
      ],
      "metadata": {
        "id": "m1wdPte0t-RN"
      },
      "id": "m1wdPte0t-RN"
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.precision', 2)\n",
        "\n",
        "df = pd.DataFrame(training_stats)\n"
      ],
      "metadata": {
        "id": "XKpimY16Daxk"
      },
      "id": "XKpimY16Daxk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the data frame as cvs and excel files:"
      ],
      "metadata": {
        "id": "mfd6T1tvW7sG"
      },
      "id": "mfd6T1tvW7sG"
    },
    {
      "cell_type": "code",
      "source": [
        "path = FOLDER_PATH + 'training_stats.cvs'\n",
        "df.to_csv(path)\n",
        "\n",
        "path = FOLDER_PATH + 'training_stats.xlsx'\n",
        "df.to_excel(path)"
      ],
      "metadata": {
        "id": "pp1s488UFMbD"
      },
      "id": "pp1s488UFMbD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display the dataframe to see what it looks like:"
      ],
      "metadata": {
        "id": "4qY4i4-VUdbr"
      },
      "id": "4qY4i4-VUdbr"
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "5-D5375yEI9f"
      },
      "id": "5-D5375yEI9f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compare Folds"
      ],
      "metadata": {
        "id": "RdGVcmf9ls3p"
      },
      "id": "RdGVcmf9ls3p"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Group the training stats dataframe by folds and filter out losses columns:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1PPTAuc5fSmD"
      },
      "id": "1PPTAuc5fSmD"
    },
    {
      "cell_type": "code",
      "source": [
        "folds_loss = df.groupby('fold')[['total_train_loss','avg_train_loss','total_val_loss', 'avg_val_loss']].mean()\n",
        "folds_loss = folds_loss.reset_index()"
      ],
      "metadata": {
        "id": "FAHih3RkePd0"
      },
      "id": "FAHih3RkePd0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folds_loss"
      ],
      "metadata": {
        "id": "3sZFidvYkX4l"
      },
      "id": "3sZFidvYkX4l",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the losses of each fold:"
      ],
      "metadata": {
        "id": "dZU5S1Aqmckg"
      },
      "id": "dZU5S1Aqmckg"
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.lineplot(x='fold', y='total_train_loss', data=folds_loss, marker='o', label='Total Train Loss')\n",
        "sns.lineplot(x='fold', y='avg_train_loss', data=folds_loss, marker='o', label='Average Train Loss')\n",
        "sns.lineplot(x='fold', y='total_val_loss', data=folds_loss, marker='o', label='Total Val Loss')\n",
        "sns.lineplot(x='fold', y='avg_val_loss', data=folds_loss, marker='o', label='Average Val Loss')\n",
        "\n",
        "\n",
        "plt.title('Training and Validation Losses Across Folds', fontsize=20)\n",
        "\n",
        "plt.xlabel('Fold',fontsize=16)\n",
        "plt.xticks(folds_loss['fold'], fontsize=12)\n",
        "plt.ylabel('Loss',fontsize=16)\n",
        "plt.yticks(fontsize=12)\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left',fontsize = 12)\n",
        "plt.grid(axis='y')\n",
        "\n",
        "plt.savefig(FOLDER_PATH+'Training_Validation_Losses_Across_Folds.png', format='png', transparent = True, bbox_inches='tight',pad_inches=0.1)"
      ],
      "metadata": {
        "id": "rJqQ8L1Atuz4"
      },
      "id": "rJqQ8L1Atuz4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Group the training stats data frame by folds and filter out the validation accuracy column:"
      ],
      "metadata": {
        "id": "5yPHgHoDnTec"
      },
      "id": "5yPHgHoDnTec"
    },
    {
      "cell_type": "code",
      "source": [
        "folds_acc = df.groupby('fold')['val_accuracy'].mean()\n",
        "folds_acc = folds_acc.reset_index()"
      ],
      "metadata": {
        "id": "O3I3T0lmXl6v"
      },
      "id": "O3I3T0lmXl6v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folds_acc"
      ],
      "metadata": {
        "id": "ad_Lx4kqnG_7"
      },
      "id": "ad_Lx4kqnG_7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot validation accuracy across folds:"
      ],
      "metadata": {
        "id": "ivd349F4nd9u"
      },
      "id": "ivd349F4nd9u"
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "sns.lineplot(x='fold', y='val_accuracy', data=folds_acc, marker='o')\n",
        "\n",
        "\n",
        "plt.title('Validation Accuracy Across Folds',fontsize=20)\n",
        "\n",
        "plt.xlabel('Fold', fontsize = 16)\n",
        "plt.xticks(folds_acc['fold'],fontsize=12)\n",
        "plt.ylabel('')\n",
        "plt.yticks([50,55,60,65,70,75,80,85,90,95,100],fontsize=12)\n",
        "plt.grid(axis='y')\n",
        "\n",
        "plt.savefig(FOLDER_PATH+'Validation_Accuracy_Across_Folds.png', format='png', transparent = True, bbox_inches='tight',pad_inches=0.1)"
      ],
      "metadata": {
        "id": "d2BrVYaong5R"
      },
      "id": "d2BrVYaong5R",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot f1-macro scores across folds:"
      ],
      "metadata": {
        "id": "KExsZeYvgY35"
      },
      "id": "KExsZeYvgY35"
    },
    {
      "cell_type": "code",
      "source": [
        "folds_f1 = df.groupby('fold')['f1_macro'].mean()\n",
        "folds_f1 = folds_f1.reset_index()"
      ],
      "metadata": {
        "id": "BKUeV0ocgu2E"
      },
      "id": "BKUeV0ocgu2E",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folds_f1"
      ],
      "metadata": {
        "id": "nWc0Wg1cgxF8"
      },
      "id": "nWc0Wg1cgxF8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "sns.lineplot(x='fold', y='macro f1-score', data=folds_acc, marker='o')\n",
        "\n",
        "\n",
        "plt.title('Macro F1-Score Across Folds',fontsize=20)\n",
        "\n",
        "plt.xlabel('Fold', fontsize = 16)\n",
        "plt.xticks(folds_f1['fold'],fontsize=12)\n",
        "plt.ylabel('')\n",
        "plt.yticks([50,55,60,65,70,75,80,85,90,95,100],fontsize=12)\n",
        "plt.grid(axis='y')\n",
        "\n",
        "plt.savefig(FOLDER_PATH+'Validation_Accuracy_Across_Folds.png', format='png', transparent = True, bbox_inches='tight',pad_inches=0.1)"
      ],
      "metadata": {
        "id": "7PXSApj8gYNp"
      },
      "id": "7PXSApj8gYNp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compare Epochs of The Best Fold"
      ],
      "metadata": {
        "id": "-ki-PkrPlxYo"
      },
      "id": "-ki-PkrPlxYo"
    },
    {
      "cell_type": "code",
      "source": [
        "folds_f1 = df.groupby('fold')['f1_macro'].mean()\n",
        "best = folds_f1.idxmax()\n",
        "best"
      ],
      "metadata": {
        "id": "GbAeph4QuDdG"
      },
      "id": "GbAeph4QuDdG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folds_f1"
      ],
      "metadata": {
        "id": "yGAZdLgnu0mR"
      },
      "id": "yGAZdLgnu0mR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = df[df['fold'] == best].groupby('epoch')[['total_train_loss','avg_train_loss','total_val_loss', 'avg_val_loss']].mean()\n",
        "epochs = epochs.reset_index()\n",
        "epochs"
      ],
      "metadata": {
        "id": "b4Wwe3aMqLIc"
      },
      "id": "b4Wwe3aMqLIc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare epoch losses of the best fold:"
      ],
      "metadata": {
        "id": "vG1W_KDci5pR"
      },
      "id": "vG1W_KDci5pR"
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.lineplot(x='epoch', y='total_train_loss', data=epochs, marker='o', label='Total Train Loss')\n",
        "sns.lineplot(x='epoch', y='total_val_loss', data=epochs, marker='o', label='Total Val Loss')\n",
        "\n",
        "plt.title('Total Losses Across Epochs', fontsize=20)\n",
        "\n",
        "plt.xlabel('Epoch',fontsize=16)\n",
        "plt.xticks(epochs['epoch'],fontsize=12)\n",
        "plt.ylabel('Loss',fontsize=16)\n",
        "plt.yticks(fontsize=12)\n",
        "\n",
        "plt.legend(loc='upper right',fontsize = 12)\n",
        "plt.grid(axis='y')\n",
        "\n",
        "plt.savefig(FOLDER_PATH+'Total_Losses_Across_Epochs.png', format='png', transparent = True, bbox_inches='tight',pad_inches=0.1)"
      ],
      "metadata": {
        "id": "Lyxii6AFr9Mk"
      },
      "id": "Lyxii6AFr9Mk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.lineplot(x='epoch', y='avg_train_loss', data=epochs, marker='o', label='Average Train Loss')\n",
        "sns.lineplot(x='epoch', y='avg_val_loss', data=epochs, marker='o', label='Average Val Loss')\n",
        "\n",
        "\n",
        "plt.title('Average Losses Across Epochs', fontsize=20)\n",
        "\n",
        "plt.xlabel('Epoch',fontsize=16)\n",
        "plt.xticks(epochs['epoch'],fontsize=12)\n",
        "plt.ylabel('Loss',fontsize=16)\n",
        "plt.yticks(fontsize=12)\n",
        "\n",
        "plt.legend(loc='upper right',fontsize = 12)\n",
        "plt.grid(axis='y')\n",
        "\n",
        "plt.savefig(FOLDER_PATH+'Average_Losses_Across_Epochs.png', format='png', transparent = True, bbox_inches='tight',pad_inches=0.1)"
      ],
      "metadata": {
        "id": "wDWbo0oNriWw"
      },
      "id": "wDWbo0oNriWw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate the Best Fold on Test Set"
      ],
      "metadata": {
        "id": "3whBNBlYmAyU"
      },
      "id": "3whBNBlYmAyU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we evaluate the trained model of the best fold, using the same method used to evaluate the pre-trained model. \\\n",
        "We do this to see if there was significant improvement after being trained with the curated dataset."
      ],
      "metadata": {
        "id": "EeTf4ipDAQ_W"
      },
      "id": "EeTf4ipDAQ_W"
    },
    {
      "cell_type": "code",
      "source": [
        "best_fold_path = 'fold_' + str(best) + '.bin'\n",
        "best_fold_path"
      ],
      "metadata": {
        "id": "tK1hHfn5hbwK"
      },
      "id": "tK1hHfn5hbwK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load best model:\n",
        "if os.path.exists(best_fold_path):\n",
        "    model.load_state_dict(torch.load(best_fold_path))\n",
        "\n",
        "else:\n",
        "  print(f\"Error: {best_fold_path} does not exist\")"
      ],
      "metadata": {
        "id": "c5u642My0b4D"
      },
      "id": "c5u642My0b4D",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Test Batches"
      ],
      "metadata": {
        "id": "4zOF8xqiEbIL"
      },
      "id": "4zOF8xqiEbIL"
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "for i, test_batch in enumerate(test_dataloader):\n",
        "  test_status = 'Batch ' + str(i+1) + ' / ' + str(len(test_dataloader))\n",
        "  print(test_status, end='\\r')\n",
        "\n",
        "  b_input_ids = test_batch[0].to(device)\n",
        "  b_input_mask = test_batch[1].to(device)\n",
        "  b_token_type_ids = test_batch[2].to(device)\n",
        "\n",
        "\n",
        "  outputs = model(b_input_ids,\n",
        "                  token_type_ids=None,\n",
        "                  attention_mask=b_input_mask,\n",
        "                  return_dict = False)\n",
        "\n",
        "  preds = outputs[0]\n",
        "  test_preds = preds.detach().cpu().numpy()\n",
        "\n",
        "\n",
        "  if i == 0:  # first batch\n",
        "    stacked_test_preds = test_preds\n",
        "\n",
        "  else:\n",
        "    stacked_test_preds = np.vstack((stacked_test_preds, test_preds))\n",
        "\n",
        "\n",
        "# Round predictions to either 0 or 1\n",
        "final_preds = np.argmax(stacked_test_preds, axis=1)"
      ],
      "metadata": {
        "id": "P0brIi0ZAoHX"
      },
      "id": "P0brIi0ZAoHX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Performance Scores"
      ],
      "metadata": {
        "id": "KFq5FYy-D0k2"
      },
      "id": "KFq5FYy-D0k2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Like before, we can calculate the model's performance scores using the function that was previously defined. \\"
      ],
      "metadata": {
        "id": "UohH-1T5n9GV"
      },
      "id": "UohH-1T5n9GV"
    },
    {
      "cell_type": "code",
      "source": [
        "labels = df_test[RELEVANCE]\n",
        "\n",
        "report, accuracy = classification_report_with_accuracy_score(final_preds, labels)\n",
        "# report is a DataFrame\n",
        "print(report)"
      ],
      "metadata": {
        "id": "RjztZbIqDSYC"
      },
      "id": "RjztZbIqDSYC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the missclassified tweets:"
      ],
      "metadata": {
        "id": "h1a8h_yHoBUm"
      },
      "id": "h1a8h_yHoBUm"
    },
    {
      "cell_type": "code",
      "source": [
        "posts = df_test[POST]\n",
        "ids = df_test[POST_ID]"
      ],
      "metadata": {
        "id": "3Mv3XE72pjFl"
      },
      "id": "3Mv3XE72pjFl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "posts"
      ],
      "metadata": {
        "id": "X3xWam32p_9D"
      },
      "id": "X3xWam32p_9D",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids"
      ],
      "metadata": {
        "id": "MOmFu0CjqAr-"
      },
      "id": "MOmFu0CjqAr-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wrong_preds = []\n",
        "\n",
        "for i in range(len(labels)):\n",
        "  predicted = final_preds[i]\n",
        "  correct = labels[i]\n",
        "\n",
        "  if predicted != label:\n",
        "    info = {'post id': ids[i],\n",
        "            'post content': posts[i],\n",
        "            'prediction': predicted,\n",
        "            'correct label': correct}\n",
        "    wrong_preds.append(info)"
      ],
      "metadata": {
        "id": "G156Pnq5oEFI"
      },
      "id": "G156Pnq5oEFI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(wrong_classification)\n",
        "df"
      ],
      "metadata": {
        "id": "fYLbGWFrqI_S"
      },
      "id": "fYLbGWFrqI_S",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = FOLDER_PATH + 'wrong_predictions.cvs'\n",
        "df.to_csv(path)\n",
        "\n",
        "path = FOLDER_PATH + 'wrong_predictions.xlsx'\n",
        "df.to_excel(path)"
      ],
      "metadata": {
        "id": "tCXZ9o5RqYJR"
      },
      "id": "tCXZ9o5RqYJR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot report as a table and save figure as a png:"
      ],
      "metadata": {
        "id": "IsQrZd_4oSLV"
      },
      "id": "IsQrZd_4oSLV"
    },
    {
      "cell_type": "code",
      "source": [
        "without_acc = report.loc[['0', '1', 'macro avg', 'weighted avg']]\n",
        "without_acc = without_acc.rename(index={'0': 'not relevant', '1': 'relevant'})\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(2,4))\n",
        "ax.axis('off')\n",
        "table = pd.plotting.table(ax, without_acc, loc='right',cellLoc='center',rowLoc='center')\n",
        "table.auto_set_font_size(False)\n",
        "table.set_fontsize(14)\n",
        "table.scale(2.9, 3)\n",
        "plt.title('BERTimbau Performance After Fine-Tuning',\n",
        "           loc='left',fontsize=20)\n",
        "acc_str = 'Overall Accuracy: '+ str(accuracy.round(decimals=2))\n",
        "fig.text(0.67, 0.08, acc_str, ha='center', fontsize=14)\n",
        "\n",
        "plt.savefig(FOLDER_PATH+'PerformanceTable_TrainedModel.png', format='png', transparent = True, bbox_inches='tight',pad_inches=0.1)"
      ],
      "metadata": {
        "id": "bNXcNheXDwTR"
      },
      "id": "bNXcNheXDwTR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the confusion matrix to visualize the model's predictions against their true value, and save the figure as a png:"
      ],
      "metadata": {
        "id": "drqDckTND3Xf"
      },
      "id": "drqDckTND3Xf"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "confusion_matrix = confusion_matrix(df_test[RELEVANCE], final_preds)\n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "ax = sns.heatmap(confusion_matrix, annot=True,cmap=\"rocket_r\", fmt = '.0f',\n",
        "            yticklabels=['not relevant', 'relevant'],\n",
        "            xticklabels=['not relevant', 'relevant'],square=True,\n",
        "                 annot_kws={\"size\": 14},cbar=True,\n",
        "                 cbar_kws={'shrink': 0.9})\n",
        "# ax.set_font\n",
        "# plt.yticks(rotation=0)\n",
        "plt.xticks(fontsize=12)  # X-axis tick labels font size\n",
        "plt.yticks(fontsize=12)\n",
        "# Add title and labels\n",
        "plt.title('BERTimbau Performance After Fine-Tuning', fontsize=22, loc='center',\n",
        "          pad=20)\n",
        "plt.xlabel('Predicted Labels', fontsize=16)\n",
        "plt.ylabel('True Labels', fontsize=16)\n",
        "\n",
        "plt.savefig(FOLDER_PATH+'ConfusionMatrix_TrainedModel.png', format='png', transparent = True, bbox_inches='tight',pad_inches=0.1)\n"
      ],
      "metadata": {
        "id": "r64_E_E8D4J0"
      },
      "id": "r64_E_E8D4J0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GrXC9A7sSIP3"
      },
      "id": "GrXC9A7sSIP3",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "8-3218n_elhK"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}